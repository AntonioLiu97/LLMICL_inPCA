nohup: ignoring input
PyTorch threads: 4
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.14s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.80s/it]
dict_keys(['uncorrelated_random_PDF_l_0.05_13b_5.pkl', 'uncorrelated_random_PDF_l_0.1_13b_22.pkl', 'uncorrelated_random_PDF_l_0.1_13b_15.pkl', 'uncorrelated_random_PDF_l_0.05_13b_7.pkl', 'uncorrelated_random_PDF_l_0.05_13b_4.pkl', 'uncorrelated_random_PDF_l_0.02_13b_5.pkl', 'uncorrelated_random_PDF_l_0.1_13b_14.pkl', 'uncorrelated_random_PDF_l_0.05_13b_1.pkl', 'uncorrelated_random_PDF_l_0.02_13b_9.pkl', 'uncorrelated_random_PDF_l_0.05_13b_2.pkl', 'uncorrelated_random_PDF_l_0.1_13b_21.pkl', 'uncorrelated_random_PDF_l_0.1_13b_9.pkl', 'uncorrelated_random_PDF_l_0.02_13b_0.pkl', 'uncorrelated_random_PDF_l_0.1_13b_24.pkl', 'uncorrelated_random_PDF_l_0.02_13b_1.pkl', 'uncorrelated_random_PDF_l_0.05_13b_0.pkl', 'uncorrelated_random_PDF_l_0.1_13b_19.pkl', 'uncorrelated_random_PDF_l_0.1_13b_23.pkl', 'uncorrelated_random_PDF_l_0.1_13b_18.pkl', 'uncorrelated_random_PDF_l_0.05_13b_3.pkl', 'uncorrelated_random_PDF_l_0.1_13b_13.pkl', 'uncorrelated_random_PDF_l_0.02_13b_8.pkl', 'uncorrelated_random_PDF_l_0.02_13b_4.pkl', 'uncorrelated_random_PDF_l_0.05_13b_9.pkl', 'uncorrelated_random_PDF_l_0.05_13b_6.pkl', 'uncorrelated_random_PDF_l_0.02_13b_7.pkl', 'uncorrelated_random_PDF_l_0.1_13b_16.pkl', 'uncorrelated_random_PDF_l_0.1_13b_17.pkl', 'uncorrelated_random_PDF_l_0.02_13b_6.pkl', 'uncorrelated_random_PDF_l_0.02_13b_3.pkl', 'uncorrelated_random_PDF_l_0.1_13b_8.pkl', 'uncorrelated_random_PDF_l_0.05_13b_8.pkl', 'uncorrelated_random_PDF_l_0.1_13b_12.pkl', 'uncorrelated_random_PDF_l_0.1_13b_20.pkl', 'uncorrelated_random_PDF_l_0.02_13b_2.pkl', 'uncorrelated_random_PDF_l_0.1_13b_25.pkl'])
dict_keys([])
Processing  uncorrelated_random_PDF_l_0.02_13b_0.pkl
Traceback (most recent call last):
  File "/home/admin-quad/Toni/LLMICL_inPCA/models/generate_predictions_1.py", line 183, in <module>
    PDF_list = calculate_multiPDF(full_series, prec, mode = mode, refine_depth = refine_depth, llama_size = llama_size)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/admin-quad/Toni/LLMICL_inPCA/models/generate_predictions_1.py", line 112, in calculate_multiPDF
    out = model(batch['input_ids'].cuda(), use_cache=True)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1038, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 925, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 635, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/admin-quad/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 389, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 558.00 MiB (GPU 0; 15.73 GiB total capacity; 14.60 GiB already allocated; 334.62 MiB free; 15.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
